\documentclass[conference, a4paper]{IEEEtran}
% 引入库
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}

\begin{document}
\title{A U-Net Based Discriminator for Conditional Deep Convolutional Generative Adversarial Networks
}

\author{\IEEEauthorblockN{KaiXu Chen}
	\IEEEauthorblockA{{Kanazawa University} \\
		Kakuma-machi 920-1192\\
		Kanazawa, Japan \\
		xchen@csl.ec.t.kanazawa-u.ac.jp}
	\and
	\IEEEauthorblockN{Satoshi Yamane}
	\IEEEauthorblockA{{Kanazawa University} \\
		Institute of Science and Engineering\\
		Kakuma-machi 920-1192\\
		Kanazawa, Japan \\
		syamane@is.t.kanazawa-u.ac.jp}
}
\maketitle

\begin{abstract}
	Generative adversarial network (GAN) \cite{a} is a prevalent generative model. Deep convolutional generative adversarial network (DCGAN) \cite{c}, based on traditional generative adversarial networks, introduces convolutional neural networks (CNN) into the training for unsupervised learning to improve the effect of generative networks. Conditional generative adversarial network (CGAN) \cite{b} is a conditional model which adds condition extension into GAN. In this paper, we present a new generative model called conditional-DCGAN (C-DCGAN), is a combination of DCGAN and CGAN, which integrates the feature extraction of convolutional networks and condition auxiliary generative sample for image recognition.The result of simulation experiments shows that this model can improve the accuracy of image recognition.
	 
	And to target the issue which the GAN is the capacity to synthesize globally and locally coherent images with object shapes and textures indistinguishable from real images \cite{e},we proposed U-Net \cite{d} based architecutre allows to provide detailed per-pixed feedback to the generator while maintaining the golbal coherence of synthesized images,by providing the global image feedback as well.
	Empowered by the per-pixel response of the discriminator, we further propose a per-pixel consistency regularization technique based on the CutMix data augmentation, encouraging the U-Net discriminator to focus more on semantic and structural changes between real and fake images. With the per-pixel consistency regularization technique, it improves the U-Net discriminator training, further enhancing the quality of generated samples.
	
	We evaluated our method with Fashion-MNIST, CelebA, and
	CIFAR-10 and obtained enlightening results.
\end{abstract}

%\IEEEpeerreviewmaketitlewe 

\begin{IEEEkeywords}
	Generative adversarial Network(GAN), convolutional nerual networks(CNN), U-Net, conditation-GAN, DCGAN, generative model, image recognition
\end{IEEEkeywords}
 
\section{Introduction}
The quality of synthetic images produced by generative adversarial networks(GANs) has seen tremendous improvement recently.
Generative Adversarial Nets were introduced as a novel way to train generative models.
In an unconditioned generative model, there is no control on modes of the data being generated.However, by conditioning the model on additional information it is possible to direct the data generation process. Such conditioning could be based on class labels, on some part of data for inpainting like, or even on data from different modality.So the Conditional GAN introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data,y,to condition on to both the generator and discriminator.

The DCGAN propose that one way to build
good image representations is by training Generative Adversarial Networks (GANs), and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks.The DCGAN introduce a class of CNNs called deep convolutional generative adversarial networks,that have certain architectural constraints, and they are a strong candidate fo unsupervised learning.

In this paper, we present a new combination network,called conditional-DCGAN, which integrates the feature extraction of convolutional networks and condition auxiliary generative sample for image recognition. The result of simulation experiments shows that this model can improve the accuracy of image recognition.

In the GANs,one source of the problem lies potentially in the discriminator network. The discriminator aims to model the data distribution, acting as a loss function to provide the generator a learning signal to synthesize realistic image samples.The stronger the discriminator is, the better the generator has to become. In the current state-of-the-art GAN models, the discriminator being a classification network learns only a representation that allows to efficiently penalize the generator based on the most discriminative difference between real and synthetic images. Thus, it often focuses either on the global structure or local details. The problem amplifies as the discriminator has to learn in a non-stationary environment: the distribution of synthetic samples shifts as the generator constantly changes through training, and is prone to forgetting previous tasks.

To mitigate this problem, we propose an alternative dis-
criminator architecture, which outputs simultaneously both
global (over the whole image) and local (per-pixel) deci-
sion of the image belonging to either the real or fake class. Motivated by the ideas from the segmentation
literature, we re-design the discriminator to take a role of both a classifier and segmenter. We change the architecture of the discriminator network to a U-Net,where the encoder module performs per-image classification, as in the standard GAN setting, and the decoder module outputs per-pixel class decision, providing spatially coherent feedback to the generator. This architectural change leads to a stronger discriminator, which is encouraged to maintain a more powerful data representation, making the generator task of fooling the discriminator more challenging and thus improving the quality of generated samples.We do not modify the
generator in any way, and our work is to use U-Net architecture into the discriminator,which based on combine the DCGAN and CGAN.

We evaluate the proposed U-Net conditional-DCGAN model across several datasets using the state-of-the-art dataset as a baseline and observe an improved quality of the generated samples in terms of the FID and IS metrics.

Our contributions can be summarized as follows:
\begin{itemize}
	\item[-] We propose a new conditional-DCGAN (C-DCGAN) model which integrates the feature extraction of convolutional networks and condition auxiliary generative sample for image recognition.
	\item[-] We re-design the discriminator to take a role of both a classifier and segmenter. We change the architecture of the discriminator network to a U-Net, where the encoder module performs per-image classification, as in the standard GAN setting, and the decoder module outputs per-pixel class decision, providing spatially coherent feedback to the generator.
	\item[-] We compared the performance of different network structures on different datasets.Such as U-Net CGAN, U-Net DCGAN, and simple CGAN and simple DCGAN in some datasets.
\end{itemize}




\clearpage
\section*{Ackonwledgement}
We are fortunate and thankful for all the advice and guidance we have received during this work, especially that of my professor, Satoshi Yamane, and my classmate. Additionally we would like to thank all of the folks at CSL for providing support, resources, and conversations. We’d like to thank Nvidia for them product, a Titan-X GPU used in this work.

\begin{thebibliography}{1}	
	\bibitem{a} 1.	Goodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair,Sherjil, Courville, Aaron C., and Bengio, Yoshua. Generative adversarial nets. NIPS, 2014.
	\bibitem{b} Mirza M, Osindero S. Conditional generative adversarial nets. arXiv 2014[J]. arXiv preprint arXiv:1411.1784, 2014.
	\bibitem{c} Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks[J]. arXiv preprint arXiv:1511.06434, 2015.
	\bibitem{d} Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241.
	\bibitem{e} Schonfeld E, Schiele B, Khoreva A. A u-net based discriminator for generative adversarial networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 8207-8216.
	\bibitem{f} Dong H, Neekhara P, Wu C, et al. Unsupervised image-to-image translation with generative adversarial networks[J]. arXiv preprint arXiv:1701.02676, 2017.
	\bibitem{g} Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.
	\bibitem{h} Karras T, Laine S, Aila T. A style-based generator architecture for generative adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2019: 4401-4410.

	%\bibliographystyle{IEEEtran}
	%\bibliography{reference}
\end{thebibliography}

\end{document}